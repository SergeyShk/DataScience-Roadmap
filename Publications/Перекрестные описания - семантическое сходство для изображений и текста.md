За последнее десятилетие удалось добиться значительного прогресса в области [автоматического аннотирования изображений](https://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html) - задачи, в которой компьютерный алгоритм создает письменные описания изображений. Значительный прогресс достигнут благодаря использованию современных методов глубокого обучения, разработанных как для задач компьютерного зрения, так и для обработки естественного языка, в сочетании с [большими наборами данных](https://ai.googleblog.com/2018/09/conceptual-captions-new-dataset-and.html), которые объединяют изображения и описания, созданные людьми. Помимо поддержания таких важных практических приложений, как [предоставление описаний изображений для людей со слабым зрением](https://blog.google/outreach-initiatives/accessibility/get-image-descriptions/), эти наборы данных также позволяют работать над интересными исследовательскими вопросами, касающимися интеграции языковых и визуальных средств в качестве входных данных модели. Например, изучение глубокого представления такого слова, как «автомобиль», означает использование как лингвистического, так и визуального контекстов.

Наборы данных аннотированных изображений, содержащие пары текстовых описаний и соответствующих им изображений, такие как [MS-COCO](https://cocodataset.org/#home) и [Flickr30k](http://bryanplummer.com/Flickr30kEntities/), широко используются для изучения совместных визуальных и текстовых представлений, а также для построения моделей создания описаний изображений. К сожалению, эти наборы данных имеют ограниченные кросс-модальные соответствия: изображения не сопоставляются с другими изображениями, описания сопоставляются только с другими описаниями того же изображения (также называемыми совместными описаниями), есть пары изображение-описание, которые совпадают, но не помечены как совпадение, и нет меток, указывающих, когда пара изображение-описание не совпадает. Это подрывает исследования того, как интермодальное обучение (например, соединение описаний и изображений) влияет на интрамодальные задачи (соединение описаний с описаниями или изображений с изображениями). Это особенно важно потому, что значительный объем работы по изучению изображений в сочетании с текстом мотивируется аргументами о том, что визуальные элементы должны привносить дополнительную информацию и улучшать языковое представление.

Чтобы восполнить этот пробел, авторы представили статью «[Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO](https://www.aclweb.org/anthology/2021.eacl-main.249/)», опубликованную на [EACL 2021](https://2021.eacl.org/). Набор данных с перекрестными описаниями (Crisscrossed Captions, CxC) расширяет тестовые и валидационные поднаборы MS-COCO, добавляя оценки семантического сходства для пар изображение-текст, текст-текст и изображение-изображение. Критерии оценок основаны на [семантическом сходстве текстов](https://www.aclweb.org/anthology/S12-1051.pdf) (Semantic Textual Similarity), существующем и широко принятом показателе семантического родства между парами коротких текстов, который авторы применили также к изображениям. В целом, CxC содержит полученные человеком оценки семантического сходства для 267095 пар (полученных из 1335475 независимых оценок), что является значительным расширением по масштабу и детализации по сравнению с исходными 50 тысячами пар валидационного и тестового поднаборов MS-COCO. Авторы выложили [CxC оценки сходства](https://github.com/google-research-datasets/Crisscrossed-Captions) вместе с кодом для объединения CxC с существующими данными MS-COCO. Таким образом, любой, кто знаком с MS-COCO, может легко улучшить свои эксперименты с CxC.

![image10](https://habrastorage.org/webt/kx/iz/dy/kxizdyjkef2ke5aojjparybuu24.png)

*Набор данных CxC расширяет тестовые поднаборы из MS-COCO, добавляя полученные человеком оценки семантического сходства для существующих пар изображение-описание и совместных описаний (сплошные линии), а также увеличивает плотность оценок, добавляя человеческие оценки для новых пар изображение-описание, описание-описание и изображение-изображение (пунктирные линии).*

# Создание набора данных CxC

«Одна картинка стоит тысячу слов», - эта знаменитая фраза, вероятно, указывает на то множество деталей и их взаимосвязей, которые обычно можно найти на изображениях. Мы можем описать текстуру меха у собаки, назвать логотип на фрисби, за которым она гонится, подметить выражение лица человека, который только что бросил фрисби, или заметить лист ярко-красного цвета, висящий на дереве над головой человека, и так далее.

Набор данных CxC расширяет тестовые поднаборы MS-COCO с помощью дополнительной разметки оценок семантического сходства пар соответствий внутри и между модальностями. MS-COCO имеет пять описаний для каждого изображения, [разделенных](https://arxiv.org/abs/1412.2306) на поднаборы для обучения (размером 410 тыс. описаний - 82 тыс. изображений), для валидации (25 тыс. описаний – 5 тыс. изображений) и тестирования (25 тыс. описаний – 5 тыс. изображений). Идеальным расширением набора данных было бы оценить каждую пару в нем (описание-описание, изображение-изображение и изображение-описание), но это невозможно, так как для этого потребовалось бы получить ручную разметку для миллиардов пар.

Учитывая, что случайно выбранные пары изображений и описаний, вероятно, будут отличаться друг от друга, авторы придумали способ выбора элементов для человеческой разметки, чтобы она включала по крайней мере несколько новых пар с высоким ожидаемым сходством. С целью уменьшить зависимость выбранных пар от моделей, используемых для их поиска, авторы ввели схему косвенной выборки (изображенную ниже), в которой они кодируют изображения и текстовые описания, используя разные методы кодирования, и вычисляют сходство между парами одинаковых элементов модальности, составляя таким образом матрицы сходства. Изображения кодируются с использованием эмбеддингов [Graph-RISE](https://arxiv.org/abs/1902.10814), а описания кодируются двумя методами: [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175) (USE) и усреднением [мешка слов](https://en.wikipedia.org/wiki/Bag-of-words_model) (bag-of-words, BoW) на основе эмбеддингов [GloVe](https://nlp.stanford.edu/pubs/glove.pdf). Поскольку каждый пример MS-COCO имеет пять совместных описаний, авторы усреднили их эмбеддинги для создания одного представления для каждого примера, что позволяет гарантировать, что все пары описаний могут быть сопоставлены с парами изображений (подробнее о том, как авторы выбирали интермодальные пары, ниже).

![image2](https://habrastorage.org/webt/hr/hg/pe/hrhgpele9pacpzny6dxmc5px_ly.png)

***Сверху**: матрица сходства текста (каждая ячейка соответствует оценке сходства)
построена с использованием усредненных эмбеддингов совместных описаний, поэтому каждая текстовая запись соответствует одному изображению, в результате чего получается матрица 5000x5000. Использовались два разных метода кодирования текста, но для простоты показана только одна матрица сходства текста. **Снизу**: матрица оценок сходства изображений для каждого изображения в наборе данных, в результате получается матрица 5000x5000.*

Следующим шагом алгоритма косвенной выборки является использование вычисленных оценок сходства изображений для получения смещенной выборки пар описаний для ручной разметки (и наоборот). Например, выберем два описания с высоким сходством из матрицы оценок сходства текстов, затем возьмем каждое из их парных изображений, в результате чего получается новая пара изображений, которые отличаются по внешнему виду, но похожи по тому, что они изображают, на основе их описаний. Например, для описаний «Собака застенчиво смотрит в сторону» и «Черная собака поднимает голову в сторону, чтобы насладиться ветерком» модель покажет достаточно высокое сходство, поэтому соответствующие изображения двух собак на рисунке ниже могут быть выбраны для оценки схожести изображений. Этот этап также можно начать и с двух изображений с высоким вычисленным сходством, чтобы получить новую пару описаний. Таким образом, авторы опосредованно отобрали новые интрамодальные пары - и, по крайней мере, некоторые из них очень похожи - для которых они получили ручную разметку.

![image7](https://habrastorage.org/webt/th/lu/qp/thluqpp7aoddybjjdcy_6uiijhc.png)

***Сверху**: пары изображений выбираются на основе вычисленного сходства описаний. **Снизу**: пары описаний выбираются на основе вычисленного сходства изображений, которые они описывают.*

Наконец, авторы используют эти новые интрамодальные пары и оценки их сходства, размеченные вручную, чтобы выбрать новые интермодальные пары для ручной разметки. Авторы используют существующие пары изображение-описание для связи между модальностями. Например, если пара описаний ij была оценена людьми как очень похожие, выбираем изображение из примера i и описание из примера j, чтобы получить новую интермодальную пару для ручной разметки. И снова используются интрамодальные пары с наивысшей оценкой сходства для выборки, потому что они включают, по крайней мере, некоторое количество новых очень похожих пар. Наконец, в набор данных добавляется ручная разметка оценок сходства для всех существующих интермодальных пар и большой выборки совместных описаний.

В следующей таблице приведены примеры пар семантического сходства изображения (semantic image similarity, SIS) и семантического сходства изображения-текста (semantic image-text similarity, SITS), соответствующих значению сходства, где 5 указывает на наиболее похожие пары, а 0 - полностью несхожие.

![image3](https://habrastorage.org/webt/iu/ce/hm/iucehmwbu0qgkcnkbgwqt5g3mbm.png)
![image3(1)](https://habrastorage.org/webt/dj/t7/zn/djt7znltp8vss9z3dc3ywlxq4uy.png)
![image6](https://habrastorage.org/webt/4q/ai/bg/4qaibg54bq667gn1hvowyytyth8.png)
![image2(1)](https://habrastorage.org/webt/ox/tn/-a/oxtn-amhbkswpyl0nz0v9vv0ybc.png)
![image1](https://habrastorage.org/webt/53/rl/3w/53rl3wqv9fqkn9tkhfinqcj2cd0.png)
![image15](https://habrastorage.org/webt/sz/nj/oj/sznjojadw1chmivga_dd--vmdgc.png)

*Примеры для каждой полученной человеком оценки сходства (**слева**: значение оценки сходства от 5 до 0, где 5 - очень похожи и 0 - совершенно не похожи) пар изображений на основе задач SIS (**в центре**) и SITS (**справа**). Стоит отметить, что эти примеры предназначены для иллюстративных целей и не входят в набор данных CxC.*

# Результаты

MS-COCO поддерживает три задачи извлечения:

1.  Для данного изображения найти соответствующие описания среди всех других описаний в тестовом наборе.
2.	Для данного описания найти соответствующее изображение среди всех других изображений в тестовом наборе.
3.	Для данного описания найти другие его совместные описания среди всех других описаний в тестовом наборе.

Пары MS-COCO неполные, потому что описания, созданные для одного изображения, иногда одинаково хорошо применимы и к другому, но эти соответствия не фиксируются в наборе данных. CxC расширяет эти существующие задачи извлечения новыми положительными парами, а также поддерживает новую задачу извлечения изображения-изображения. Благодаря своим оценкам сходства, CxC также позволяет измерять корреляции между значениями сходства, полученными моделью и вручную. Метрики для задачи извлечения в основном сосредоточены только на положительных парах, в то время как показатели корреляции CxC дополнительно учитывают относительный порядок схожих элементов, а также включают элементы с низкой оценкой (несоответствия). Поддержка таких оценок на общем наборе изображений и описаний делает их более ценными для понимания интермодального обучения по сравнению с несвязанными наборами соответствий описание-изображение, описание-описание и изображение-изображение.

Авторы провели серию экспериментов, чтобы показать полезность рейтингов CxC. Для этого они построили три модели двойного энкодера (dual encoder, DE), используя [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)-base в качестве текстового энкодера и [EfficientNet-B4](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html) в качестве энкодера изображений:

1.	Модель текст-текст (DE_T2T), в которой для обеих сторон используется общий текстовый энкодер.
2.	Модель изображение-текст (DE_I2T), которая использует вышеупомянутые энкодеры текста и изображения и включает в себя один слой поверх текстового энкодера текста, приводящий в соответствие выход последнего выходным данным энкодера изображения.
3.	Мультизадачная модель (DE_I2T+T2T), обученная на взвешенной комбинации задач текст-текст и изображение-текст.

![image11](https://habrastorage.org/webt/cr/j3/o1/crj3o1pzr0rnppea6m07euozcyg.png)

*Результаты на задаче извлечений CxC - сравнение моделей двойного энкодера текст-текст (T2T), изображение-текст (I2T) и мультизадачной модели (I2T+T2T) для всех четырех задач извлечений.*

Из полученных результатов видно, что DE_I2T+T2T (желтый столбец) работает лучше, чем DE_I2T (красный столбец) в задачах извлечения пар изображение-текст и текст-изображение. Таким образом, добавление интрамодальной (текст-текст) обучающей задачи помогло улучшить результаты для интермодальных задач (изображение-текст, текст-изображение). Что касается двух других интрамодальных задач (текст-текст и изображение-изображение), DE_I2T+T2T демонстрирует хорошие стабильные результаты на них обеих.

![image6 (1)](https://habrastorage.org/webt/o1/qb/n7/o1qbn7fcmpgmkg5pcl5qmiy7kw4.png)

*Результаты корреляции CxC для моделей, показанных выше.*

Для задач корреляции DE_I2T показывает лучшие результаты на SIS, а DE_I2T+T2T лучше всех справилась в целом со всеми задачами. Оценки корреляции также показывают, что DE_I2T хорошо работает только с изображениями: у него самый высокий SIS, но гораздо худший STS. Добавление функции потери на парах текст-текст к обучению DE_I2T (DE_I2T+T2T) дает более стабильные общие результаты.

[Набор данных CxC](https://github.com/google-research-datasets/Crisscrossed-Captions) обеспечивает гораздо более полный набор взаимосвязей между изображениями и описаниями, чем необработанные пары изображение-описание MS-COCO. [Опубликованы](https://github.com/google-research-datasets/Crisscrossed-Captions) новые оценки сходства, подробности о которых можно найти в [статье](https://www.aclweb.org/anthology/2021.eacl-main.249/). Авторы надеются побудить исследовательское сообщество разрабатывать новые модели, превосходящие результаты, представленных CxC, на основе совместного обучения интер- и интрамодальным представлениям.

# Авторы

* **Автор оригинала** – Zarana Parekh, Jason Baldridge
* **Перевод** – [Смирнова Екатерина](https://habr.com/ru/users/smekur/)
* **Редактирование и вёрстка** – [Шкарин Сергей](https://habr.com/ru/users/kouki_rus/)